### 第六周作业

## 复习题

# 第二题
PageRank算法背后的设计思想主要基于以下几点：

一、网页链接结构分析
PageRank算法最初是为互联网页面设计的，其核心思想是通过分析网页之间的链接关系来确定网页的权重和排名。它将整个互联网视为一个巨大的有向图，其中每个网页是一个节点，而超链接则是从一个页面指向另一个页面的有向边。基于这个视角，可以构建一个随机游走模型，也就是一阶马尔可夫链，用于量化网页的重要性。

二、随机游走模型
PageRank算法基于有向图上的随机游走模型，描述了一个随机游走者如何沿着图的边随机移动，从一个节点访问到另一个节点。在满足某些条件的前提下，这个随机游走过程最终会收敛到一个平稳分布。在这个平稳分布中，每个节点被访问的概率即为其PageRank值，这个值可以被解释为节点的重要性。

三、递归定义与迭代计算
PageRank是递归定义的，这意味着一个页面的PageRank值部分地取决于链接到它的其他页面的PageRank值。因此，计算PageRank值通常需要迭代方法。算法会为每个网页分配一个初始的权重值（通常是相等的），然后通过迭代计算来不断更新这些权重值。在每一轮迭代中，PageRank算法将网页的权重从其入链的网页平均分配给出链的网页，这是一个基于链接关系的传递性过程。

四、阻尼因子的引入
为了模拟用户在浏览网页时的随机跳转行为，PageRank引入了一个阻尼因子（damping factor），通常设置为约0.85。阻尼因子表示用户有一定的概率随机跳转到任何一个网页，而不是按照链接关系浏览。这一设定使得PageRank算法更加符合实际用户的浏览行为，并提高了算法的鲁棒性和准确性。

五、重要性与排名的量化
PageRank算法通过量化网页的重要性，为搜索引擎提供了一种评估网页在搜索结果中重要性的方法。网页的权重值被用作排名因素，按照权重值从高到低进行排序，以决定其在搜索引擎结果中的位置。这使得搜索引擎能够提供更具相关性和质量的搜索结果。

综上所述，PageRank算法背后的设计思想是基于网页链接结构的分析、随机游走模型的构建、递归定义与迭代计算、阻尼因子的引入以及重要性与排名的量化。这些思想共同构成了PageRank算法的核心，使其成为评估网页重要性的有效工具。
# 第三题
贝叶斯定理是关于随机事件A和B的条件概率（或边缘概率）的一则定理。以下是贝叶斯定理的内容及其重要应用：

贝叶斯定理的内容
贝叶斯定理的表达式为：P(A|B)=P(A)×P(B|A)/P(B)。其中，P(A|B)是在B发生的情况下A发生的概率，也称作A的后验概率；P(A)是A的先验概率（或边缘概率），即不考虑任何B方面的因素时A发生的概率；P(B|A)是已知A发生后B的条件概率，也称作B的后验概率；P(B)是B的先验概率或边缘概率。

贝叶斯定理的重要应用
医疗诊断：在医疗领域，贝叶斯原理可以帮助医生根据病人的症状和检测结果来估计疾病的概率。通过结合病人的年龄、性别、病史和实验室测试结果，医生可以使用贝叶斯定理来更新对某种疾病诊断的概率。
机器学习：贝叶斯原理是许多机器学习算法的基础，特别是在处理不确定性和噪声数据时。贝叶斯分类器和贝叶斯网络是两种常见的应用，它们可以帮助计算机根据观察到的数据来预测未知数据的概率分布。
经济学：在经济学中，贝叶斯原理可以用来分析市场行为和预测经济变量。例如，经济学家可以使用贝叶斯方法来估计消费者对价格变化的反应，或者预测政策变化对经济指标的影响。
信号处理：在无线通信和信号处理领域，贝叶斯原理用于估计信号的参数，如频率、相位和幅度。这些估计对于解调、编码和信道均衡等任务至关重要。
搜索引擎：搜索引擎使用贝叶斯原理来改进搜索结果的相关性。通过分析用户的查询历史和点击行为，搜索引擎可以更新对网页相关性的概率估计，从而提供更准确的搜索结果。
财务分析：在财务分析中，贝叶斯原理可以用来评估投资的风险和回报。通过结合历史数据和专家意见，分析师可以使用贝叶斯方法来更新对不同投资策略成功概率的估计。
法律：在法律领域，贝叶斯原理可以用来评估证据的力度和更新对案件事实的信念。例如，陪审团成员可以根据新的证据和证词来调整他们对被告有罪或无罪的概率估计。
此外，贝叶斯定理还被广泛应用于垃圾邮件过滤、语音识别、推荐系统、人工智能等多个领域。通过不断更新后验概率，贝叶斯定理使得模型能够适应新的观测数据，从而提高预测和决策的准确性。这种灵活性和自适应性使得贝叶斯定理成为处理不确定性和进行推理的强大工具。
# 第四题
蒙特卡罗方法的基本原理是通过数字模拟试验，得到所要求解的出现某种事件的概率，作为问题的近似解。以下是对蒙特卡罗方法基本原理的详细阐述：

一、核心思想
蒙特卡罗方法的核心思想是利用随机数和概率统计方法来模拟问题，通过大量随机样本的采样，得到问题的概率分布或期望值。这种方法特别适用于那些无法用精确数学公式求解的问题，或者公式求解非常困难的问题。

二、基本原理
随机抽样：蒙特卡罗方法首先需要在定义的概率空间中生成随机样本。这些样本的数量通常决定了结果的精度，样本数量越多，结果越接近真实值。
模拟计算：生成随机样本后，需要将这些样本代入到目标函数中，得到目标函数的函数值。然后，根据函数值的大小关系，统计满足条件的样本数目，从而得到目标函数在采样区域内的估计值。
统计分析：最后，利用采样得到的数据，根据大数定律和中心极限定理，计算问题的期望值、方差、置信区间等统计量，并根据这些统计量进行进一步的分析和推断。
三、应用实例
蒙特卡罗方法在许多领域都有广泛的应用，以下是一些具体的实例：

物理学：蒙特卡罗方法可以用来模拟物质在不同温度、压力、浓度等条件下的行为，从而研究物质的性质和行为规律。这种方法在材料科学、地球科学、化学等领域都有广泛的应用。
金融工程：蒙特卡罗方法可以用来模拟股票价格、利率、汇率等金融变量的随机波动，并基于这些模拟结果计算出投资组合的风险和收益。这种方法可以帮助投资者更好地评估不同投资方案的风险和收益，并做出更合理的投资决策。
计算数学：在高维空间中，蒙特卡罗方法可以用于计算复杂函数的积分。此外，它还可以用于求解微分方程、积分方程、特征值计算和非线性方程组等数学问题。
机器学习：蒙特卡罗方法可以用来训练神经网络和优化模型参数。例如，蒙特卡洛树搜索算法可以用来训练游戏人工智能系统。
生物医学：蒙特卡罗方法可以用来模拟光子在生物组织中的传输和散射过程，从而研究组织的结构和功能，以及光学成像和治疗等方面的应用。
四、优点与局限性
蒙特卡罗方法的优点是简单易懂，不需要对问题的具体结构做出太多的假设，而且可以直接利用计算机生成大量随机数进行计算，解决了许多传统方法难以解决的问题。然而，蒙特卡罗方法也存在一定的局限性，其计算结果可能存在一定的误差，因为估计值是通过随机样本计算得到的。因此，在实际应用中需要考虑样本数量、采样方式、计算精度等因素，以得到可靠的计算结果。

综上所述，蒙特卡罗方法是一种基于随机抽样的计算方法，其基本原理是通过大量随机样本的采样来估计问题的解。这种方法在许多领域都有广泛的应用，但也需要根据具体问题的特点和需求来选择合适的采样方式和计算精度。
# 第五题
梯度下降法，简单来说，就是一种寻找函数最小值（或局部最小值）的方法。它就像是你站在一个山的某个位置，想要尽快下山，于是决定走一步算一步，每走到一个位置时，都求解当前位置的梯度，然后沿着梯度的负方向（也就是当前最陡峭的位置向下走）迈出一步，这样一直走下去，最终可能会到达山脚（或某个局部最低点）。

下面用更通俗的语言详细解释梯度下降法的基本原理：

确定方向：
梯度是一个矢量，它指向函数值增长最快的方向。换句话说，如果你站在一个点上，梯度会告诉你哪个方向是“上坡”（函数值增加）的，哪个方向是“下坡”（函数值减少）的。
在梯度下降法中，我们想要找到函数的最小值，所以应该沿着梯度的反方向（也就是下坡方向）移动。
确定步长：
每次移动多远，这取决于一个叫做“学习率”的参数。学习率是一个正数，它决定了每一步的大小。
如果学习率太大，可能会一步跨过最低点，导致在最低点附近来回振荡，甚至可能永远无法到达最低点。
如果学习率太小，虽然每一步都朝着正确的方向移动，但移动得太慢，需要很长时间才能到达最低点。
迭代更新：
从一个初始点开始，计算该点的梯度。
根据梯度和学习率，确定下一步的位置。
到达新的位置后，再次计算梯度，并更新位置。
重复这个过程，直到满足某个终止条件（比如梯度值小于某个阈值，或者达到预设的迭代次数）。
终止条件：
梯度下降法需要一个终止条件来停止迭代。常见的终止条件包括梯度值小于某个阈值（这意味着已经接近最低点了），或者达到预设的迭代次数（这意味着已经尝试了足够多的步骤）。
注意事项：
梯度下降法不一定能找到全局最小值，特别是当函数有多个局部最小值时。它可能会找到一个局部最小值并停止。
初始点的选择对结果有影响。不同的初始点可能会导致找到不同的局部最小值。
学习率的选择非常关键。太大的学习率可能导致无法收敛，太小的学习率可能导致收敛速度过慢。
总的来说，梯度下降法是一种简单而有效的优化算法，它利用梯度信息来逐步更新参数，使得目标函数的值逐渐减小。虽然它有一些局限性（比如可能找不到全局最小值），但在许多实际问题中仍然非常有用。


## 践习题

# 第一题

```python
# import numpy as np

# # 生成 100 个符合标准正态分布的样本
# samples = np.random.normal(0, 1, 100)
# print(samples)
import numpy as np
import matplotlib.pyplot as plt

# 生成 100 个符合标准正态分布的样本
samples = np.random.normal(0, 1, 100)

print(samples)#打印这些样本

# 绘制直方图
plt.hist(samples, bins=20, density=True, alpha=0.6, color='g')

# 设置图表标题和坐标轴标签
plt.title('Standard Normal Distribution Samples')
plt.xlabel('Value')
plt.ylabel('Density')

# 显示图形
plt.show()
```



# 第二题
这个和第一题我用的同样的代码，直接实现两套功能
```python
# import numpy as np

# # 生成 100 个符合标准正态分布的样本
# samples = np.random.normal(0, 1, 100)
# print(samples)
import numpy as np
import matplotlib.pyplot as plt

# 生成 100 个符合标准正态分布的样本
samples = np.random.normal(0, 1, 100)

print(samples)#打印这些样本

# 绘制直方图
plt.hist(samples, bins=20, density=True, alpha=0.6, color='g')

# 设置图表标题和坐标轴标签
plt.title('Standard Normal Distribution Samples')
plt.xlabel('Value')
plt.ylabel('Density')

# 显示图形
plt.show()
```

# 第三题

```python

import numpy as np

# 输入四个整数代表二阶矩阵
a11, a12, a21, a22 = map(int, input("请输入四个整数，代表二阶矩阵（按顺序为 11、12、21、22 位置的值）： ").split())

matrix = np.array([[a11, a12], [a21, a22]])

eigenvalues, eigenvectors = np.linalg.eig(matrix)

print("特征值为：", eigenvalues)
print("特征向量为：")
for i in range(len(eigenvalues)):
    print(eigenvectors[:, i])

```

# 第五题

```python

import numpy as np

data = np.array([[1, 2, 3],
                 [1, -1, 4],
                 [2, 1, 3],
                 [1, 3, -1]])

# 去掉第一行的标识行
data = data[1:]

covariance_matrix = np.cov(data)

print("协方差矩阵为：")
print(covariance_matrix)

```

# 补充题

```python

import numpy as np
import matplotlib.pyplot as plt

def function(x):
    return 0.25 * (x - 0.5)**2 + 1

def derivative(x):
    return 0.5 * (x - 0.5)

x = np.linspace(-2, 2, 400)
y = function(x)

# 梯度下降参数
learning_rate = 0.1
num_iterations = 100
current_x = 1.5
iterations = []
values = []

for i in range(num_iterations):
    gradient = derivative(current_x)
    current_x = current_x - learning_rate * gradient
    iterations.append(i)
    values.append(current_x)
    print(f"Iteration {i}: x = {current_x}")

print(f"Final result: x = {current_x}")

plt.plot(x, y, label='Function')
plt.scatter(values, function(np.array(values)), c='r', label='Iteration Points')
plt.xlabel('x')
plt.ylabel('y')
plt.title('Gradient Descent for Function 0.25*(x - 0.5)^2 + 1')
plt.legend()
plt.show()


```